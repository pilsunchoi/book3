{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 17 설문조사 요인분석"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [17.1 요인분석이란?](#17.1-요인분석이란?)\n",
    "- [17.2 항공사 서비스 설문조사 요인분석](#17.2-항공사-서비스-설문조사-요인분석)\n",
    "- [17.3 성격 측정 설문조사 요인분석](#17.3-성격-측정-설문조사-요인분석)\n",
    "- [부록: 크론바흐 알파](부록:-크론바흐-알파)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**요인분석**(factor analysis)의 목표는 서로 관련된 변수들을 **요인**(factor)이라고 불리는 더 적은 수의 변수로 만들려는 것이다. 여러 변수들의 배후에 소수의 \"**잠재적**(latent)\" 요인이 존재한다는 아이디어에 기반하고 있다. 가령 설문조사 전체 문항(즉 변수)을 몇 개로 그룹핑하여 변수의 수를 줄이려는 경우 유용하다. 요인분석은 심리 측정, 성격 이론, 생물학, 마케팅, 제품 관리, 경영과학, 재무, 기계학습 등 다양한 분야에서 사용된다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17.1 요인분석이란?\n",
    "\n",
    "### 개요\n",
    "\n",
    "**요인분석**은 어떤 관측된 변수들, 그러면서 서로 관련성을 지닌 것으로 간주되는 변수들 간의 변동성을 **요인**이라고 하는 관측되지 않은, 그리고 숫자가 더 적은 변수로 설명하는 데 사용되는 통계적 방법이다.(Wikipedia) 예를 들어, 6개의 관측된 변수의 변동(variation)이 2개의 관측되지 않은 (배후의) 변수의 변동에 의해 잘 표현되는 상황이 여기에 해당한다. \n",
    "\n",
    "데이터 세트에 여러 관련 변수들이 체계적인(systematic) 상호의존성(inter-dependency)을 나타낼 때, 공통성(commonality)을 생성하는 잠재적 요인을 찾는 기법이다. 요인분석에서 관측된 변수는 잠재적 요인과 \"오차\" 항의 선형 조합으로 모델링된다. 어떤 변수의 **요인 부하량**(factor loading)은 해당 변수가 해당 요인과 어느 정도 관련되는지를 나타낸다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 요인분석 모델\n",
    "\n",
    "$n$개의 개체(관측치)가 있고, 관측된 변수가 $p$개인 데이터 세트를 생각해보자. 요인분석은 ($p$개보다 작은) $k$개의 공통 요인($f_{i,j}$)을 사용하여 $p$개 관측변수를 설명하려고 한다. 각 개체는 $k$개의 고유한 공통 요인을 가지며, 이것들이 관측들과 연결된다. 그 관계를 나타낸 것이 요인 **부하량 행렬**($L\\in \\mathbb R^{p \\times k}$)이다. 어떤 하나의 관측치($x_{i,m}$)를 다음과 같이 모형화한다. \n",
    "\n",
    "$$\n",
    "x_{i,m}-\\mu_{i} = l_{i,1}f_{1,m}+\\cdots+l_{i,k}f_{k,m}+\\varepsilon_{i,m}\n",
    "$$\n",
    "\n",
    "여기에서 $i=1,...,p$이고, $m=1,...,n$이며, 각 기호는 다음을 의미한다.\n",
    "- $x_{i,m}$ : $m$번째 개체의 $i$번째 관측변수 값\n",
    "- $ \\mu_{i}$ : $i$번째 관측변수의 평균,\n",
    "- $l_{i,j}$ : $j$번째 요인의 $i$번째 관측변수에 대한 부하량,\n",
    "- $f_{j,m}$ : $m$번째 개체의 $j$번째 요인의 값,\n",
    "- $\\varepsilon_{i,m}$ : 평균이 0이고 분산이 유한한 $(i,m)$번째의 관측되지 않는 확률적 오차항."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 예시\n",
    "\n",
    "어떤 심리학자가 사람들에게는 \"언어적 지능\"과 \"수학적 지능\"의 두 가지 종류의 지능이 있으며 어느 쪽도 직접 관찰되지 않는다는 가설을 세운다. 이 상황에서 1,000명의 학생이 10개의 서로 다른 학문 분야에 대해 시험을 치렀고, 그 점수가 있다고 하자. 각 학생이 대규모 모집단에서 무작위로 선택되는 경우 각 학생의 10개 분야 점수는 확률변수이다. \n",
    "\n",
    "해당 심리학자는 \"언어적 지능\"과 \"수학적 지능\"의 두 가지 종류의 지능에 추가적으로 다음과 같은 가설을 세운다. 즉 10개 학문 분야 각각에 있어서 각 학생들의 예상(expected) 점수는 다음과 같이 결정된다는 것이다.\n",
    "\n",
    "$$\n",
    "{l_1 × \\text{학생의 언어적 지능}} + {l_2 × \\text{학생의 수학적 지능}}.\n",
    "$$\n",
    "\n",
    "즉 어떤 숫자($l_1$) 곱하기 언어적 지능 수준, 거기에다 또 다른 숫자($l_2$) 곱하기 수학적 지능 수준이다. 다시 말하면 언어적 지능과 수학적 지능이라는 두 개 \"요인\"의 선형 조합(linear combination)이라는 것이다. 어떤 과목에서 학생들의 예상 점수를 얻기 위해 두 종류의 지능에 곱해지는 숫자를 \"요인 부하량\"이라고 한다. 예를 들어 천문학 분야에서 학생들의 예상 성적은 가령 다음과 같다는 가설이다.\n",
    "\n",
    "\n",
    "$$\n",
    "{10 × \\text{학생의 언어적 지능}} + {6 × \\text{학생의 수학적 지능}}.\n",
    "$$\n",
    "\n",
    "여기에서 숫자 10과 6은 천문학과 관련된 요인 부하량이다. 다른 학문 과목은 다른 요인 부하량을 가질 수 있음은 물론이다. 어떤 두 학생의 언어 및 수학적 지능의 정도가 동일한 경우라도 천문학 분야의 개별 성적은 서로 다를 수 있다. 왜냐하면 위의 식은 예상 성적이고 실제의 성적은 다를 수밖에 없으며, 측정 오류도 있을 수 있기 때문이다. 이러한 차이를 총칭하여 \"오류(error)\"라고 부른다.  \n",
    "\n",
    "위 사례에서 요인분석에 들어가는 관찰 가능한 데이터는 학생 1,000명 각각의 10개 과목 점수, 즉 총 10,000개의 숫자이다. 이 데이터를 통해 두 가지 지능의 요인 부하량을 추론하게 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 탐색적 vs 확인적 요인분석\n",
    "\n",
    "**탐색적 요인분석**(EFA: exploratory factor analysis)은 측정된 변수 간의 기본 관계를 **찾아내는** 것이 가장 중요한 목표인 요인분석 기법이다. 가령 \"스트레스 지수\"와 같은 어떤 척도(scale)를 측정하는 데 사용되며, 측정된 변수의 배후에 있는 잠재적 구성요인(latent construct)를 식별하는 데 사용된다. 연구자가 측정변수(measured variable)의 요인이나 패턴에 대해 선험적 가설을 갖고 있지 않을 때 사용한다. \n",
    "\n",
    "**확인적 요인분석**(CFA: confirmatory factor analysis)은 요인분석의 한 형태로서 주로 구성요인(또는 요인)의 측정값이 해당 요인의 특성에 대한 연구자의 생각과 일치하는지 여부를 테스트하는 데, 즉 **확인하는** 데 사용된다. 데이터가 측정 모델에 대한 가설과 맞는지 여부를 테스트하는 것으로서, 이 가설은 이론이나 선행 연구를 기반으로 한다. CFA는 Jöreskog(1969)에 의해 처음 개발되었다.\n",
    "\n",
    "탐색적 요인분석(EFA)과 확인적 요인분석(CFA)은 둘 다 잠재적 구성요인(또는 요인)에 기인하는 것으로 여겨지는 측정변수의 공유 분산(shared variance)을 이해하는 데 사용된다. 그러나 이러한 유사성에도 불구하고 EFA와 CFA는 개념적으로나 통계적으로 구별되는 분석이다. EFA의 목표는 데이터를 기반으로 요인을 식별하고 그것에 의해 설명되는 분산의 양을 최대화하는 것이다. 연구자는 얼마나 많은 요인이 나타날 것이며, 이러한 요인이 어떤 항목이나 변수를 구성할 것인지에 대한 특정 가설을 미리 갖고 있을 필요가 없다. 이에 반해, CFA는 선험적 가설을 평가하고, 주로 이론에 의해 주도된다. CFA 분석을 위해서는 연구자가 사전에 요인의 수, 이러한 요인의 상관관계 여부, 어떤 항목/측정값이 어떤 요인에 적재되고 반영되는지를 가정해야 한다. 따라서 모든 부하량이 자유롭게 결정되는 탐색적 요인분석과 달리 CFA는 특정 부하량을 0으로 명시적으로 제약하기도 한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17.2 항공사 서비스 설문조사 요인분석\n",
    "\n",
    "\n",
    "- 자료: [Factor Analysis Tutorial](https://towardsdatascience.com/factor-analysis-a-complete-tutorial-1b7621890e42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터\n",
    "\n",
    "여기에서 예시적으로 사용된 데이터는 어떤 **항공사 승객 만족도** 데이터 세트로, [Kaggle](https://www.kaggle.com/teejmahal20/airline-passenger-satisfaction)에서 다운로드 받을 수 있다. 게시돼 있는 [압축파일](https://www.kaggle.com/datasets/teejmahal20/airline-passenger-satisfaction/download?datasetVersionNumber=1)을 다운로드 받아 압축을 푼 다음, 2개의 파일 중 \"train.csv\" 파일을 사용하기로 한다.(파일명을 \"airline.csv\"으로 바꿈) 데이터 세트는 어떤 항공사의 승객 만족도 설문조사 결과이다. 103,904개의 관측값과 25개의 열이 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../Data/airline.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 데이터 파일이 들어 있는 디렉토리를 적는 방법에 대해서는 \u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# 2.2.1절의 ISLR \"College.csv\"를 불러들이는 방법을 참조할 것\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m airline \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../Data/airline.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m airline\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m    947\u001b[0m )\n\u001b[0;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    602\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    604\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 605\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1439\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1733\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1734\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1736\u001b[0m     f,\n\u001b[0;32m   1737\u001b[0m     mode,\n\u001b[0;32m   1738\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1739\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1740\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1741\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1742\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1743\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1744\u001b[0m )\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    851\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    852\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    853\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    854\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    855\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 856\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    857\u001b[0m             handle,\n\u001b[0;32m    858\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    859\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    860\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    861\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    862\u001b[0m         )\n\u001b[0;32m    863\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    864\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    865\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../Data/airline.csv'"
     ]
    }
   ],
   "source": [
    "# 데이터 파일이 들어 있는 디렉토리를 적는 방법에 대해서는 \n",
    "# 2.2.1절의 ISLR \"College.csv\"를 불러들이는 방법을 참조할 것\n",
    "airline = pd.read_csv('../Data/airline.csv')\n",
    "airline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airline = airline.iloc[:, 1:] # 불필요한 첫번째 열을 제거\n",
    "pd.set_option('display.max_columns', None) # 모든 컬럼이 나오게 함\n",
    "airline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airline.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터세트 변수\n",
    "\n",
    "\n",
    "- `Gender`: 승객의 성별(Female, Male)\n",
    "\n",
    "- `Customer Type`: 고객 유형(Loyal customer, disloyal customer)\n",
    "\n",
    "- `Age`: 승객의 나이\n",
    "\n",
    "- `Type of Travel`: 승객의 비행 목적(Personal Travel, Business Travel)\n",
    "\n",
    "- `Class`: 승객의 비행기 여행 클래스(Business, Eco, Eco Plus)\n",
    "\n",
    "- `Flight distance`: 해당 여행의 비행 거리\n",
    "\n",
    "- `Inflight wifi service`: 기내 와이파이 서비스 만족도(0:해당 없음, 1-5)\n",
    "\n",
    "- `Departure/Arrival time convenient`: 출발/도착 시간 만족도\n",
    "\n",
    "- `Ease of Online booking`: 온라인 예약 만족도\n",
    "\n",
    "- `Gate location`: 게이트 위치 만족도\n",
    "\n",
    "- `Food and drink`: 식음료 만족도\n",
    "\n",
    "- `Online boarding`: 온라인 탑승 만족도\n",
    "\n",
    "- `Seat comfort`: 좌석 만족도\n",
    "\n",
    "- `Inflight entertainment`: 기내 엔터테인먼트 만족도\n",
    "\n",
    "- `On-board service`: 온보드 서비스 만족도\n",
    "\n",
    "- `Leg room service`: 레그룸 서비스 만족도\n",
    "\n",
    "- `Baggage handling`: 수하물 처리 만족도\n",
    "\n",
    "- `Check-in service`: 체크인 서비스 만족도\n",
    "\n",
    "- `Inflight service`: 기내 서비스 만족도\n",
    "\n",
    "- `Cleanliness`: 청결도 만족도\n",
    "\n",
    "- `Departure Delay in Minutes`: 출발 지연 시간(분)\n",
    "\n",
    "- `Arrival Delay in Minutes`: 도착 지연 시간(분)\n",
    "\n",
    "- `Satisfaction`: 항공사 만족도(만족, 중립, 불만족)\n",
    "\n",
    "이 데이터 세트는 John D([US Airline passenger satisfaction survey](https://www.kaggle.com/johndddddd/customer-satisfaction))의 데이터 세트를 일부 수정한 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 상관계수 행렬\n",
    "\n",
    "\n",
    "요인분석의 첫 번째 단계는 모든 변수 간의 상관관계를 살펴보고, 어떤 변수가 다른 변수와 지나치게 높은 상관관계가 있는지 확인하는 것이다. 일부 변수, 특히 설문조사 답변과 관련된 변수는 매우 높은 상관관계가 있을 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airline.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 상관계수 행렬 히트맵\n",
    "\n",
    "변수가 많기 때문에 히트맵을 통해 상관계수가 높은 변수를 시각적으로 파악한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "c= airline.corr()\n",
    "sns.heatmap(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 결과를 보면, `Departure Delay in Minutes`와 `Arrival Delay in Minutes` 사이의 매우 높은 상관성(0.98)이 발견된다. 왜 그런지 이해할 수 있을 것이다. 비행기가 일정보다 늦게 출발하면 도착 시간도 늦어질 것이기 때문이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 요인분석 대상 14개 관측변수\n",
    "\n",
    "총 24개 변수 중 14개는 비행편의 다양한 측면(기내 Wi-Fi 서비스, 음식 및 음료, 온라인 탑승, 좌석 편안함 등)에 대해 1점에서 5점까지의 리커트 척도로 고객의 응답을 기록한 것이다. 다음과 같은 14개 변수가 요인분석 대상이다. \n",
    "\n",
    "  - `Inflight wifi service`\n",
    "  - `Departure/Arrival time convenient`\n",
    "  - `Ease of Online booking`\n",
    "  - `Gate location`\n",
    "  - `Food and drink`\n",
    "  - `Online boarding`\n",
    "  - `Seat comfort`\n",
    "  - `Inflight entertainment`\n",
    "  - `On-board service`\n",
    "  - `Leg room service`\n",
    "  - `Baggage handling`\n",
    "  - `Checkin service`\n",
    "  - `Inflight service`\n",
    "  - `Cleanliness`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 요인분석 실행\n",
    "\n",
    "어떻게 14개의 변수를 더 적은 수의 잠재변수(요인)로 만들 수 있을까? 먼저 필요한 패키지를 설치하고 가져와야 한다. 즉 요인분석에 필요한 [factor_analyzer package](https://factor-analyzer.readthedocs.io/en/latest/factor_analyzer.html)를 설치한다.(처음에 한 번 설치하면, 그 다음부터는 다시 설치할 필요 없다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install factor_analyzer  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from factor_analyzer import FactorAnalyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eigenvalue\n",
    "\n",
    "우선 얼마나 많은 요인이 필요한지 알아내기 위해 고유값(eigenvalue)을 검토한다. 고유값은 어떤 요인이 관련 변수의 분산을 어느 정도 설명하는지를 측정한 것이다. 어떤 요인의 고유값이 1보다 크면 그 요인이 어떤 하나의 변수보다 더 많은 분산을 설명한다는 의미이다. 가령 어떤 요인의 고유값이 2.5이면, 해당 요인이 2.5개 변수의 분산을 설명할 수 있음을 의미한다.\n",
    "\n",
    "`FactorAnalyzer` 함수에서 우리는 원하는 요인의 수와 회전(rotation) 유형을 지정한다. 여기에서 회전이란 더 간단하고 해석가능한 구조를 달성하기 위해 요인을 회전시키는 것을 의미한다. 많은 유형의 회전이 있는데, 아래에서는 `varimax` 회전을 사용한다. 이는 생성된 요인이 상관성이 없는지(orthogonality: 직교성) 확인하면서 부하량 제곱의 분산의 합을 최대화하는 방식이다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14개 관측변수에 대해 10개의 요인을 추출\n",
    "x = airline[airline.columns[7:21]] \n",
    "fa = FactorAnalyzer(10, rotation='varimax')\n",
    "fa.fit(x)\n",
    "\n",
    "# 10개 요인의 고유값을 구해 그림으로 그림\n",
    "ev, v = fa.get_eigenvalues()\n",
    "print(ev)\n",
    "plt.plot(range(1,x.shape[1]+1),ev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 요인의 개수 및 부하량\n",
    "\n",
    "위 그림에서 보듯이 3번째 요인 이후부터 고유값이 크게 떨어지기 때문에 여기서는 3개의 요인만 사용하기로 한다. 이들 3개 요인의 고유값은 3.8, 2.3, 2.2로서 다 합치면 약 8.3으로서 8.3개 변수의 분산을 설명하는 것을 의미한다. 이렇게 했을 때, 어떤 요인이 만들어지는지 살펴보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fa = FactorAnalyzer(3, rotation='varimax')\n",
    "fa.fit(x)\n",
    "loads = fa.loadings_\n",
    "loads = pd.DataFrame(loads)\n",
    "loads.columns = ['Factor 1', 'Factor 2', 'Factor 3']\n",
    "loads.index = [list(range(1,15)), x.columns]\n",
    "loads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "어떤 변수의 요인 부하량이 높을수록 해당 요인에서 해당 변수가 더 중요하다. 여기에서 보통 0.5의 부하량 컷오프가 사용된다. 이 컷오프는 어떤 변수가 어떤 요인에 속하는지 결정한다. 이 기준에 따르면, 위 표에서 첫 번째 요인에는 변수 5, 7, 8, 14(각각 0.75, 0.78, 0.74, 0.85의 부하량)가 포함된다.\n",
    "\n",
    "\n",
    "다음은 생성된 세 가지 요인과 거기에 포함된 변수들이다. 세 가지 요인에 대한 가능한 해석(interpretability)으로서 \"Comfort\", \"Service\", \"Convenience\"로 명명했다.\n",
    "\n",
    "  1. Comfort(편안함): Food and Drink, Seat comfort, Inflight entertainment, Cleanliness\n",
    "  \n",
    "  1. Service(서비스): Onboard service, Baggage Handling, Inflight Service\n",
    "  \n",
    "  1. Convenience(편의성): In flight Wifi, Departure/Arrival time convenience, Online Booking, Gate Location."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cronbach alpha\n",
    "\n",
    "다음으로 크론바흐 알파는 요인의 변수들이 일관되고(coherent) 신뢰할 만한지를 측정하는 데 사용될 수 있다.(**부록** 참조) 우선 크론바흐 알파 계산에 필요한 [pingouin 패키지](https://pingouin-stats.org/)를 설치하고, 버전을 업데이트한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pingouin\n",
    "# !pip install --upgrade pingouin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pingouin as pg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "크론바흐 알파가 대략 0.6이나 0.7 이상이면 신뢰할만한 것으로 간주된다.(**부록** 주의점 참조) 다음은 `pingouin` 패키지의 `cronbach_alpha` 함수를 사용하여 크론바흐 알파를 계산하는 코드이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 요인별 변수 지정\n",
    "factor1 = airline[['Food and drink', 'Seat comfort', 'Inflight entertainment', \n",
    "                   'Cleanliness']]\n",
    "factor2 = airline[['On-board service', 'Baggage handling', 'Inflight service']]\n",
    "factor3 = airline[['Inflight wifi service', 'Departure/Arrival time convenient', \n",
    "                   'Ease of Online booking', 'Gate location']]\n",
    "\n",
    "# 크론바흐 알파 계산\n",
    "factor1_alpha = pg.cronbach_alpha(factor1)\n",
    "factor2_alpha = pg.cronbach_alpha(factor2)\n",
    "factor3_alpha = pg.cronbach_alpha(factor3)\n",
    "\n",
    "factor1_alpha, factor2_alpha, factor3_alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3개 요인 각각에 대해 크론바흐 알파 값과 95% 신뢰구간이 나와 있다. 모두 0.7 이상으로서 각 요인들이 신뢰성 있는 변수에 의해 구성된 것을 알 수 있다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 새로운 요인 변수 만들기\n",
    "\n",
    "우리는 위에서 만든 3개의 새로운 요인을 다른 분석이나 예측을 위한 변수로 사용할 수 있다. 다음은 14개의 개별 변수에 의해 3개의 새로운 요인 변수를 만들어 그것을 데이터 프레임 형태로 만드는 코드이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_variables = fa.fit_transform(x)\n",
    "factors = pd.DataFrame(new_variables)\n",
    "factors.columns = ['Comfort', 'Service', 'Convenience']\n",
    "factors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pd.concat()` 함수를 사용하여 원래의 데이터프레임(`airline`)에 새로 만든 3개의 요인들로 구성된 데이터프레임(`factors`)을 연결한다. 아래 결과를 보면 맨마지막 열에 3개의 요인들이 추가된 것을 알 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airline1 = pd.concat([airline, factors], axis=1)\n",
    "airline1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17.3 성격 측정 설문조사 요인분석\n",
    "\n",
    "- 자료: [Introduction to Factor Analysis in Python](https://www.datacamp.com/community/tutorials/introduction-factor-analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 로딩\n",
    "\n",
    "- 성격 평가 프로젝트 데이터 세트에 대한 요인분석을 수행해 보자. 다음 [링크](https://vincentarelbundock.github.io/Rdatasets/datasets.html)에서 해당 데이터 세트를 다운로드할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from factor_analyzer import FactorAnalyzer\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psych = pd.read_csv('https://vincentarelbundock.github.io/Rdatasets/csv/psych/bfi.csv') \n",
    "psych"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 변수\n",
    "\n",
    "- International Personality Item Pool(국제 성격평가 항목 풀)에서 가져온 25개의 자기 보고식 성격평가 항목에다 추가적으로 3개의 인구통계학적 변수(성별, 교육, 연령)가 포함되어 있다. 이에 따라 총 28개 변수에 대한 2,800개의 관측치가 있다.\n",
    "\n",
    "\n",
    "- 처음 25개 성격평가 항목은 친화성(Agreeableness), 성실성(Conscientiousness), 외향성(Extraversion), 신경증(Neuroticism), 개방성(Opennness)의 5가지 요인으로 나누어 볼 수 있다.\n",
    "\n",
    "\n",
    "- 항목 데이터는 6점 응답 척도를 사용하여 수집되었다. 1 매우 아니다, 2 보통 아니다, 3 약간 아니다, 4 약간 그렇다, 5 보통 그렇다, 6 매우 그렇다.\n",
    "\n",
    "#### 변수\n",
    "- A1: 다른 사람의 감정에 무관심하다.\n",
    "- A2: 다른 사람의 안부를 물어본다.\n",
    "- A3: 다른 사람을 위로하는 방법을 알고 있다.\n",
    "- A4: 아이들을 사랑한다.\n",
    "- A5: 사람들이 편안함을 느끼도록 한다.\n",
    "- C1: 일을 엄격하게 한다. \n",
    "- C2: 모든 것이 완벽해질 때까지 한다. \n",
    "- C3: 계획에 따라 일을 한다. \n",
    "- C4: 중간 정도로 일을 한다. \n",
    "- C5: 시간을 낭비한다. \n",
    "- E1: 말을 많이 하지 않는다.\n",
    "- E2: 다른 사람에게 다가가기가 어렵다. \n",
    "- E3: 사람들을 사로잡는 방법을 알고 있다. \n",
    "- E4: 쉽게 친구를 사귈 수 있다. \n",
    "- E5: 책임을 진다. \n",
    "- N1: 쉽게 화를 낸다.\n",
    "- N2: 쉽게 짜증을 낸다.\n",
    "- N3: 기분 변화가 자주 있다. \n",
    "- N4: 종종 우울함을 느낀다. \n",
    "- N5: 쉽게 당황한다. \n",
    "- O1: 아이디어가 넘친다. \n",
    "- O2: 읽기 어려운 내용은 피한다.\n",
    "- O3: 대화를 더 높은 수준으로 이끈다. \n",
    "- O4: 일들에 대해 반성하는 시간을 가진다. \n",
    "- O5: 어떤 주제에 대해 깊이 탐구하지 않는다.\n",
    "- gender: 남성 = 1, 여성 = 2\n",
    "- education: 1 = 고등학교, 2 = 고등학교 졸업, 3 = 대학, 4 = 대학 졸업 5 = 대학원 학위\n",
    "- age: 나이"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 25개 성격 문항만 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping unnecessary columns\n",
    "psych.drop(['gender', 'education', 'age'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping missing values rows\n",
    "psych.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psych.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "psych = psych.iloc[:,1:]\n",
    "psych"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 요인 가능성 테스트\n",
    "\n",
    "- 요인분석을 수행하기 전에 데이터 세트의 \"요인 가능성(factorability)\"을 평가한다. 요인 가능성은 \"데이터 세트에서 요인을 찾을 수 있는지\"를 점검하는 것이다. 요인 가능성을 확인하는 두 가지 방법이 있다.\n",
    "\n",
    "  - 바틀렛 검정(Bartlett’s Test)\n",
    "  - 카이저-마이어-올킨 검정(Kaiser-Meyer-Olkin Test)\n",
    "\n",
    "\n",
    "- Bartlett의 구형도(sphericity) 검정은 단위행렬(identity matrix)에 대해 관측된 상관행렬(correlation matrix)을 사용하여 관측된 변수가 상호 상관되는지(intercorrelate) 여부를 확인하는 것이다. 검정이 통계적으로 유의하지 않으면 요인분석을 사용해서는 안 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from factor_analyzer.factor_analyzer import calculate_bartlett_sphericity\n",
    "chi_square_value, p_value = calculate_bartlett_sphericity(psych)\n",
    "chi_square_value, p_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이 Bartlett 검정에서 $p$값은 0.0이다. 따라서 검정은 통계적으로 유의하여 관측된 상관행렬이 단위행렬이 아님을 나타낸다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 다음으로 Kaiser-Meyer-Olkin(KMO) 검정은 관측된 모든 변수 간의 분산 비율을 추정하며, 이 비율이 낮을수록 요인분석에 더 적합하다. KMO 값의 범위는 0에서 1 사이로서, KMO 값이 0.6 미만이면 부적절한 것으로 간주된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from factor_analyzer.factor_analyzer import calculate_kmo\n",
    "kmo_all, kmo_model = calculate_kmo(psych)\n",
    "kmo_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 데이터의 전체 KMO는 0.84로서 우수한 편이다. 따라서 요인분석을 진행할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 요인 개수 선택\n",
    "\n",
    "- 요인 개수를 선택하기 위해 카이저 기준(Kaiser criterion)과 스크리 플롯(scree plot)을 사용할 수 있다. 둘 다 고유값을 기반으로 한다.\n",
    "\n",
    "\n",
    "- Kaiser 기준은 요인에 의해 설명되는 분산이 얼마나 크게 늘어나는지를 기반으로 하는 분석적 접근 방식이다. 고유값은 요인의 수를 결정하는 좋은 기준이다. 일반적으로 1보다 큰 고유값은 변수에 대한 선택 기준으로 간주된다.\n",
    "\n",
    "\n",
    "- 그래픽 접근 방식은 요인 고유값의 시각적 표현인 스크리 플롯을 기반으로 한다. 이 스크리 플롯은 곡선이 엘보우 형태가 되는 부근의 요인 개수를 선택한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create factor analysis object and perform factor analysis\n",
    "fa = FactorAnalyzer()\n",
    "fa.fit(psych)\n",
    "# Check Eigenvalues\n",
    "ev, v = fa.get_eigenvalues()\n",
    "ev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create scree plot using matplotlib\n",
    "plt.scatter(range(1,psych.shape[1]+1),ev)\n",
    "plt.plot(range(1,psych.shape[1]+1),ev)\n",
    "plt.title('Scree Plot')\n",
    "plt.xlabel('Factors')\n",
    "plt.ylabel('Eigenvalue')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 여기에서 6개 요인에 대해서만 고유값이 1보다 큰 것을 볼 수 있다. 따라서 일단 6개 요인을 선택하기로 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 요인분석 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create factor analysis object and perform factor analysis\n",
    "fa = FactorAnalyzer(6, rotation='varimax')\n",
    "fa.fit(psych)\n",
    "loads = pd.DataFrame(fa.loadings_)\n",
    "loads.index = psych.columns\n",
    "loads.columns = [\"Factor 1\", \"Factor 2\", \"Factor 3\", \"Factor 4\", \"Factor 5\", \"Factor 6\"]\n",
    "loads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 요인 1는 N1, N2, N3, N4, N5(신경증)에 대해 높은 요인 부하량을 갖는다.\n",
    "\n",
    "\n",
    "- 요인 2는 E1, E2, E3, E4, E5(외향성)에 대해 높은 요인 부하량을 갖는다.\n",
    "\n",
    "\n",
    "- 요인 3은 C1, C2, C3, C4, C5(성실성)에 대해 높은 요인 부하량을 갖는다.\n",
    "\n",
    "\n",
    "- 요인 4는 A1, A2, A3, A4, A5(친화성)에 대해 높은 요인 부하량을 갖는다.\n",
    "\n",
    "\n",
    "- 요인 5는 O1, O2, O3, O4, O5(개방성)에 대해 높은 요인 부하량을 갖는다.\n",
    "\n",
    "\n",
    "- 요인 6은 모든 변수에 대해 높은 부하량이 없으며 쉽게 해석할 수 없다. \n",
    "\n",
    "\n",
    "- 따라서 5개 요인만 취하기로 하고, 다시 한 번 요인분석을 해보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create factor analysis object and perform factor analysis\n",
    "fa = FactorAnalyzer(5, rotation='varimax')\n",
    "fa.fit(psych)\n",
    "loads = pd.DataFrame(fa.loadings_)\n",
    "loads.index = psych.columns\n",
    "loads.columns = [\"Factor 1\", \"Factor 2\", \"Factor 3\", \"Factor 4\", \"Factor 5\"]\n",
    "loads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get variance of each factors\n",
    "fa.get_factor_variance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `get_factor_variance()`는 각 요인에 대한 분산, 분산 비율(proportional variance), 누적 분산 비율을 제공한다.\n",
    "\n",
    "\n",
    "- 세번째 행의 누적 분산 비율을 보면, 총 42%의 분산이 5개 요인으로 설명되는 것을 알 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 요인분석의 장단점\n",
    "\n",
    "- 요인분석은 대규모 데이터 세트를 탐색하고 상호 연결된 연관성을 찾는 데 도움을 준다. 관측된 변수를 몇 개의 관측되지 않은 변수로 축소하거나 상호 관련된 변수 그룹을 식별하여, 가령 시장을 연구하는 경우 시장 상황을 압축하고 소비자 취향, 선호도, 문화적 영향 간의 숨겨진 관계를 찾는 데 도움이 된다. 또한 설문조사에서 설문지를 개선하는 데 도움이 되기도 한다.\n",
    "\n",
    "\n",
    "- 그러나 요인분석의 결과는 논란의 여지가 있다. 동일한 데이터 요소에 대해 둘 이상의 해석이 이루어질 수 있기 때문에 그 해석은 논쟁의 여지가 있을 수 있다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 참고: 요인분석 대 주성분 분석\n",
    "\n",
    "- 주성분 분석(principal component analysis, PCA)은 변수 간의 상관관계가 있는 다차원의 데이터를 효율적으로 저차원의 데이터로 요약하는 방법 중 하나이다. \n",
    "\n",
    "\n",
    "- PCA 성분은 분산(variance)의 최대량을 설명하는 반면, FA는 데이터의 공분산(covariance)을 설명한다.\n",
    "\n",
    "\n",
    "- PCA 성분은 서로 완전히 직교(orthogonal)하지만, FA에서는 요인이 직교할 필요가 없다.\n",
    "\n",
    "\n",
    "- PCA 성분은 관측된 변수의 선형 조합(linear combination)이고, FA에서 관측된 변수는 관측되지 않은 변수 또는 요인의 선형 조합이다.\n",
    "\n",
    "\n",
    "- PCA 성분은 해석할 수 없다. FA에서는 기본 요인에 레이블을 지정하고 해석할 수 있다.\n",
    "\n",
    "\n",
    "- PCA는 일종의 차원 축소 방법인 반면, 요인분석은 잠재변수 방법이다.\n",
    "\n",
    "\n",
    "- PCA는 요인분석의 한 유형이다. PCA는 관측적인 반면, FA는 모델링 기법이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 부록: 크론바흐 알파\n",
    "\n",
    "### 직관적 이해"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 신뢰도가 높은 응답 \n",
    "data = [{'Q1': 3, 'Q2': 3},\n",
    "        {'Q1': 3, 'Q2': 2},\n",
    "        {'Q1': 2, 'Q2': 1},\n",
    "        {'Q1': 4, 'Q2': 4}]\n",
    "df = pd.DataFrame(data)\n",
    "print(df)\n",
    "print(\"\\nCronbach's alpha =\", round(pg.cronbach_alpha(df)[0],4))\n",
    "print(\"\\nCorrelation coefficient =\", round(df['Q1'].corr(df['Q2']),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 신뢰도가 낮은 응답 \n",
    "data = [{'Q1': 3, 'Q2': 3},\n",
    "        {'Q1': 5, 'Q2': 1},\n",
    "        {'Q1': 4, 'Q2': 1},\n",
    "        {'Q1': 4, 'Q2': 4}]\n",
    "df = pd.DataFrame(data)\n",
    "print(df)\n",
    "print(\"\\nCronbach's alpha =\", round(pg.cronbach_alpha(df)[0],4))\n",
    "print(\"\\nCorrelation coefficient =\", round(df['Q1'].corr(df['Q2']),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 신뢰도가 최고인 응답???\n",
    "data = [{'Q1': 1, 'Q2': 4},\n",
    "        {'Q1': 2, 'Q2': 5},\n",
    "        {'Q1': 1, 'Q2': 4},\n",
    "        {'Q1': 2, 'Q2': 5}]\n",
    "df = pd.DataFrame(data)\n",
    "print(df)\n",
    "print(\"\\nCronbach's alpha =\", round(pg.cronbach_alpha(df)[0],4))\n",
    "print(\"\\nCorrelation coefficient =\", round(df['Q1'].corr(df['Q2']),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 신뢰도가 낮은 응답??? \n",
    "data = [{'Q1': 1, 'Q2': 5},\n",
    "        {'Q1': 2, 'Q2': 4},\n",
    "        {'Q1': 1, 'Q2': 5},\n",
    "        {'Q1': 1, 'Q2': 4}]\n",
    "df = pd.DataFrame(data)\n",
    "print(\"Cronbach's alpha =\", round(pg.cronbach_alpha(df)[0],4))\n",
    "print(\"Correlation coefficient =\", round(df['Q1'].corr(df['Q2']),4))\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 부록 2. Cronbach alpha 주의점\n",
    "출처: Wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 크론바흐(1951)의 알파(Cronbach's alpha)는 단일 실행에서의 신뢰도(즉, 고정된 시간에서 여러 항목에 대한 응답자의 신뢰도) 계수이다. 단일 실행 신뢰도 계수들 중에서 가장 유명하고 흔히 사용되지만, 최근 연구들은 이 계수가 부주의하게 사용됨으로써 쉽게 오류를 범하는 사례를 들어 이 계수를 주의해서 조건에 맞도록 사용할것을 권고한다. 이것의 대안으로 흔히 언급되는 것은 구조방정식 기반 신뢰도 계수(예: 동류신뢰도)이다.\n",
    "\n",
    "  - 통계학 및 심리측정에서 신뢰도(reliability)는 측정의 전반적인 일관성(consistency)이다. 측정값이 일관된 조건에서 유사한 결과를 생성하면 신뢰도가 높다고 한다.\n",
    "  \n",
    "  \n",
    "- 크론바흐 알파에 대한 흔한 오해 중의 하나는 0과 1 사이의 값을 갖는다는 오해이다. 정의에 의해, 신뢰도는 0보다 작을 수 없고, 1보다 클 수 없다. 많은 교과서들은 크론바흐 알파를 신뢰도와 동일시하며, 그 범위에 대해 부정확한 설명을 한다. \n",
    "\n",
    "\n",
    "- 음의 크론바흐 알파는 음의 변별력, 혹은 역항목 처리 실수 등의 이유로 발생할 수 있다. 크론바흐 알파와 달리, 구조방정식 기반 신뢰도 계수들은 항상 0보다 같거나 크다.\n",
    "\n",
    "\n",
    "- 높은 크론바흐 알파 값은 자료의 내적 일관성(internal consistency)을 보여준다는 오해이다. 내적 일관성이라는 용어는 신뢰도 문헌에서 흔히 사용되지만, 그 의미는 명확하게 정의되어 있지 않다. 이 용어는 때로는 특정한 종류의 신뢰도를 지칭하기 위해 사용되기도 하지만(예: 내적 일관성 신뢰도), 크론바흐 알파 외에 정확히 어떤 신뢰도 계수가 여기에 포함되는지는 불명확하다. 크론바흐(1951)는 내적 일관성이라는 용어를 명시적 정의를 내리지 않고 여러 맥락에서 사용하였다.\n",
    "\n",
    "\n",
    "- 신뢰도 계수가 얼마 이상이어야 하는지에 대해 가장 자주 인용되는 원천은 너낼리(Nunnally, 1967 등)의 책이다. 그러나 그의 권고 수준은 그의 의도와는 다르게 사용되고 있다. 그의 의도는 연구의 목적이나 단계에 따라 다른 신뢰도 기준을 적용하자는 것이었다. 그러나 초기 연구, 기초 연구, 응용 연구, 척도개발 연구 등 연구의 성격에 관계없이 0.7의 신뢰도 기준이 사용되고 있다. 0.7이라는 수치는 너낼리가 연구의 초기 단계에 대해서 언급한 수치로서, 학술지에 게재된 대부분의 연구는 여기에 해당하지 않는다. 0.7보다는 너낼리가 응용연구에 대해 언급한 0.8이라는 기준이 대부분의 실증연구에 더 적합하다.\n",
    "\n",
    "\n",
    "- 높은 신뢰도의 비용: 많은 교과서들은 신뢰도 값이 클수록 바람직하다고 설명한다. 높은 신뢰도의 잠재적 부작용에 대해서는 거의 논의하지 않는다. 그러나 하나를 얻기 위해서는 다른 무언가를 희생해야 한다는 원칙은 신뢰도에도 적용된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
